import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    mean_absolute_error, 
    mean_squared_error, 
    r2_score,
    mean_absolute_percentage_error,
    explained_variance_score
)
from typing import Dict, Optional, Tuple, Union
import warnings


def calcular_metricas_regresion(y_true, y_pred):
    """
    Calcula todas las mÃ©tricas de regresiÃ³n para un modelo.
    
    Args:
        y_true: Valores reales
        y_pred: Valores predichos
    
    Returns:
        dict: Diccionario con todas las mÃ©tricas
    """
    
    # Convertir a numpy arrays
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    
    # MÃ©tricas bÃ¡sicas
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)
    
    # MÃ©tricas adicionales
    try:
        mape = mean_absolute_percentage_error(y_true, y_pred) * 100
    except:
        # Calcular MAPE manualmente si sklearn no lo tiene
        mask = y_true != 0
        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100 if np.any(mask) else np.inf
    
    explained_var = explained_variance_score(y_true, y_pred)
    max_error = np.max(np.abs(y_true - y_pred))
    
    # MÃ©tricas estadÃ­sticas
    mean_true = np.mean(y_true)
    mean_pred = np.mean(y_pred)
    std_true = np.std(y_true)
    std_pred = np.std(y_pred)
    
    # CorrelaciÃ³n
    correlation = np.corrcoef(y_true, y_pred)[0, 1] if len(y_true) > 1 else np.nan
    
    # Sesgo (bias)
    bias = np.mean(y_pred - y_true)
    
    # Error relativo medio
    relative_error = np.mean(np.abs(y_true - y_pred) / np.maximum(np.abs(y_true), 1e-8)) * 100
    
    return {
        'MAE': mae,
        'MSE': mse,
        'RMSE': rmse,
        'RÂ²': r2,
        'MAPE': mape,
        'Explained_Variance': explained_var,
        'Max_Error': max_error,
        'Correlation': correlation,
        'Bias': bias,
        'Relative_Error_%': relative_error,
        'Mean_True': mean_true,
        'Mean_Pred': mean_pred,
        'Std_True': std_true,
        'Std_Pred': std_pred,
        'N_Samples': len(y_true)
    }


def crear_tabla_metricas(metricas_dict, nombre_modelo="Modelo", mostrar_estadisticas=True):
    """
    Crea una tabla formateada con las mÃ©tricas de un modelo.
    
    Args:
        metricas_dict: Diccionario con mÃ©tricas del modelo
        nombre_modelo: Nombre del modelo para mostrar
        mostrar_estadisticas: Si mostrar estadÃ­sticas descriptivas
    
    Returns:
        pd.DataFrame: Tabla con mÃ©tricas formateadas
    """
    
    # Definir orden y nombres de mÃ©tricas principales
    metricas_principales = [
        ('MAE', 'Mean Absolute Error'),
        ('MSE', 'Mean Squared Error'),
        ('RMSE', 'Root Mean Squared Error'),
        ('RÂ²', 'R-squared'),
        ('MAPE', 'Mean Absolute Percentage Error (%)'),
        ('Explained_Variance', 'Explained Variance'),
        ('Max_Error', 'Maximum Error'),
        ('Correlation', 'Correlation'),
        ('Bias', 'Bias'),
        ('Relative_Error_%', 'Relative Error (%)')
    ]
    
    # Crear DataFrame principal
    data = []
    for key, desc in metricas_principales:
        if key in metricas_dict:
            value = metricas_dict[key]
            # Formatear valores
            if key in ['RÂ²', 'Explained_Variance', 'Correlation']:
                formatted_value = f"{value:.4f}"
            elif key in ['MAPE', 'Relative_Error_%']:
                formatted_value = f"{value:.2f}%"
            elif key in ['MAE', 'MSE', 'RMSE', 'Max_Error', 'Bias']:
                formatted_value = f"{value:.4f}"
            else:
                formatted_value = f"{value:.4f}"
            
            data.append({
                'MÃ©trica': desc,
                nombre_modelo: formatted_value
            })
    
    df_metricas = pd.DataFrame(data)
    
    # Agregar estadÃ­sticas descriptivas si se solicita
    if mostrar_estadisticas:
        estadisticas = [
            ('Mean_True', 'Media Valores Reales'),
            ('Mean_Pred', 'Media Predicciones'),
            ('Std_True', 'Desv. Est. Valores Reales'),
            ('Std_Pred', 'Desv. Est. Predicciones'),
            ('N_Samples', 'NÃºmero de Muestras')
        ]
        
        data_stats = []
        for key, desc in estadisticas:
            if key in metricas_dict:
                value = metricas_dict[key]
                if key == 'N_Samples':
                    formatted_value = f"{int(value):,}"
                else:
                    formatted_value = f"{value:.4f}"
                
                data_stats.append({
                    'MÃ©trica': desc,
                    nombre_modelo: formatted_value
                })
        
        df_stats = pd.DataFrame(data_stats)
        
        # Separador
        separador = pd.DataFrame({
            'MÃ©trica': ['--- EstadÃ­sticas Descriptivas ---'],
            nombre_modelo: ['']
        })
        
        df_metricas = pd.concat([df_metricas, separador, df_stats], ignore_index=True)
    
    return df_metricas


def evaluar_modelo_regresion(y_true, y_pred, nombre_modelo="Modelo", 
                           mostrar_tabla=True, mostrar_estadisticas=True,
                           mostrar_plot=False, figsize=(10, 6)):
    """
    EvalÃºa un modelo de regresiÃ³n y muestra mÃ©tricas en tabla.
    
    Args:
        y_true: Valores reales
        y_pred: Valores predichos
        nombre_modelo: Nombre del modelo
        mostrar_tabla: Si mostrar la tabla de mÃ©tricas
        mostrar_estadisticas: Si incluir estadÃ­sticas descriptivas
        mostrar_plot: Si mostrar grÃ¡fico de predicciones vs reales
        figsize: TamaÃ±o de la figura
    
    Returns:
        tuple: (metricas_dict, df_tabla)
    """
    
    # Calcular mÃ©tricas
    metricas = calcular_metricas_regresion(y_true, y_pred)
    
    # Crear tabla
    df_tabla = crear_tabla_metricas(metricas, nombre_modelo, mostrar_estadisticas)
    
    # Mostrar tabla si se solicita
    if mostrar_tabla:
        print(f"\n{'='*60}")
        print(f"MÃ‰TRICAS DE REGRESIÃ“N - {nombre_modelo}")
        print(f"{'='*60}")
        print(df_tabla.to_string(index=False))
        print(f"{'='*60}")
    
    # Mostrar plot si se solicita
    if mostrar_plot:
        _plot_predicciones_vs_reales(y_true, y_pred, nombre_modelo, figsize)
    
    return metricas, df_tabla


def comparar_modelos_regresion(modelos_predicciones: Dict[str, np.ndarray], 
                             y_true: np.ndarray,
                             mostrar_tabla=True,
                             mostrar_plots=False,
                             ordenar_por='RÂ²',
                             ascendente=False,
                             guardar_csv=None):
    """
    Compara mÃºltiples modelos de regresiÃ³n y muestra mÃ©tricas en tabla comparativa.
    
    Args:
        modelos_predicciones: Dict con {nombre_modelo: predicciones}
        y_true: Valores reales
        mostrar_tabla: Si mostrar tabla comparativa
        mostrar_plots: Si mostrar grÃ¡ficos comparativos
        ordenar_por: MÃ©trica por la cual ordenar ('RÂ²', 'RMSE', 'MAE', etc.)
        ascendente: Si ordenar de forma ascendente
        guardar_csv: Ruta para guardar la tabla en CSV
    
    Returns:
        tuple: (dict_metricas_todos, df_comparacion)
    """
    
    print(f"\n{'='*80}")
    print("COMPARACIÃ“N DE MODELOS DE REGRESIÃ“N")
    print(f"{'='*80}")
    
    # Calcular mÃ©tricas para todos los modelos
    metricas_todos = {}
    for nombre, predicciones in modelos_predicciones.items():
        metricas_todos[nombre] = calcular_metricas_regresion(y_true, predicciones)
    
    # Crear tabla comparativa
    df_comparacion = crear_tabla_comparacion(metricas_todos, ordenar_por, ascendente)
    
    # Mostrar tabla si se solicita
    if mostrar_tabla:
        print(df_comparacion.to_string(index=False))
        print(f"\n{'='*80}")
        
        # Mostrar resumen de mejores modelos
        mostrar_mejores_modelos(df_comparacion)
    
    # Guardar CSV si se solicita
    if guardar_csv:
        df_comparacion.to_csv(guardar_csv, index=False)
        print(f"\nðŸ’¾ Tabla guardada en: {guardar_csv}")
    
    # Mostrar grÃ¡ficos si se solicita
    if mostrar_plots:
        crear_plots_comparacion(modelos_predicciones, y_true)
    
    return metricas_todos, df_comparacion


def crear_tabla_comparacion(metricas_todos, ordenar_por='RÂ²', ascendente=False):
    """
    Crea tabla comparativa de mÃºltiples modelos.
    """
    
    # MÃ©tricas principales para comparaciÃ³n
    metricas_comparacion = ['MAE', 'RMSE', 'RÂ²', 'MAPE', 'Correlation', 'Bias']
    
    # Crear DataFrame
    data = []
    for nombre_modelo, metricas in metricas_todos.items():
        fila = {'Modelo': nombre_modelo}
        for metrica in metricas_comparacion:
            if metrica in metricas:
                value = metricas[metrica]
                if metrica in ['RÂ²', 'Correlation']:
                    fila[metrica] = f"{value:.4f}"
                elif metrica == 'MAPE':
                    fila[metrica] = f"{value:.2f}%"
                else:
                    fila[metrica] = f"{value:.4f}"
            else:
                fila[metrica] = "N/A"
        
        # Agregar nÃºmero de muestras
        fila['N_Samples'] = f"{int(metricas.get('N_Samples', 0)):,}"
        data.append(fila)
    
    df = pd.DataFrame(data)
    
    # Ordenar por mÃ©trica especificada si existe
    if ordenar_por in df.columns and ordenar_por != 'Modelo':
        # Convertir a numÃ©rico para ordenar (remover % y convertir)
        col_ordenar = df[ordenar_por].str.replace('%', '').astype(float)
        df_sorted = df.iloc[col_ordenar.argsort()]
        if not ascendente:
            df_sorted = df_sorted.iloc[::-1]
        return df_sorted.reset_index(drop=True)
    
    return df


def mostrar_mejores_modelos(df_comparacion):
    """
    Muestra un resumen de los mejores modelos por mÃ©trica.
    """
    
    print("ðŸ† MEJORES MODELOS POR MÃ‰TRICA:")
    print("-" * 50)
    
    # MÃ©tricas donde menor es mejor
    metricas_menor_mejor = ['MAE', 'RMSE', 'MAPE', 'Bias']
    # MÃ©tricas donde mayor es mejor  
    metricas_mayor_mejor = ['RÂ²', 'Correlation']
    
    for metrica in metricas_menor_mejor + metricas_mayor_mejor:
        if metrica in df_comparacion.columns:
            # Convertir a numÃ©rico
            valores = df_comparacion[metrica].str.replace('%', '').astype(float)
            
            if metrica in metricas_menor_mejor:
                idx_mejor = valores.idxmin()
            else:
                idx_mejor = valores.idxmax()
            
            mejor_modelo = df_comparacion.loc[idx_mejor, 'Modelo']
            mejor_valor = df_comparacion.loc[idx_mejor, metrica]
            
            print(f"{metrica:>12}: {mejor_modelo:<20} ({mejor_valor})")


def _plot_predicciones_vs_reales(y_true, y_pred, nombre_modelo, figsize):
    """
    Crea grÃ¡fico de predicciones vs valores reales.
    """
    
    plt.figure(figsize=figsize)
    
    # Scatter plot
    plt.scatter(y_true, y_pred, alpha=0.6, s=30, edgecolors='black', linewidth=0.5)
    
    # LÃ­nea diagonal perfecta
    min_val = min(y_true.min(), y_pred.min())
    max_val = max(y_true.max(), y_pred.max())
    plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, 
             label='PredicciÃ³n Perfecta', alpha=0.8)
    
    # LÃ­nea de tendencia
    z = np.polyfit(y_true, y_pred, 1)
    p = np.poly1d(z)
    plt.plot(y_true, p(y_true), 'g-', linewidth=2, alpha=0.8, 
             label=f'Tendencia (pendiente={z[0]:.3f})')
    
    # Calcular RÂ²
    r2 = r2_score(y_true, y_pred)
    
    # Formato
    plt.xlabel('Valores Reales', fontsize=12)
    plt.ylabel('Predicciones', fontsize=12)
    plt.title(f'{nombre_modelo} - Predicciones vs Valores Reales (RÂ² = {r2:.4f})', fontsize=14)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()


def crear_plots_comparacion(modelos_predicciones, y_true, figsize=(15, 10)):
    """
    Crea grÃ¡ficos comparativos para mÃºltiples modelos.
    """
    
    n_modelos = len(modelos_predicciones)
    cols = min(3, n_modelos)
    rows = (n_modelos + cols - 1) // cols
    
    fig, axes = plt.subplots(rows, cols, figsize=figsize)
    if n_modelos == 1:
        axes = [axes]
    elif rows == 1:
        axes = axes.flatten()
    else:
        axes = axes.flatten()
    
    for idx, (nombre, y_pred) in enumerate(modelos_predicciones.items()):
        ax = axes[idx]
        
        # Scatter plot
        ax.scatter(y_true, y_pred, alpha=0.6, s=20)
        
        # LÃ­nea diagonal
        min_val = min(y_true.min(), y_pred.min())
        max_val = max(y_true.max(), y_pred.max())
        ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, alpha=0.8)
        
        # RÂ² y RMSE
        r2 = r2_score(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        
        ax.set_title(f'{nombre}\nRÂ² = {r2:.4f}, RMSE = {rmse:.4f}')
        ax.set_xlabel('Valores Reales')
        ax.set_ylabel('Predicciones')
        ax.grid(True, alpha=0.3)
    
    # Ocultar subplots vacÃ­os
    for idx in range(n_modelos, len(axes)):
        axes[idx].set_visible(False)
    
    plt.tight_layout()
    plt.show()


# FunciÃ³n de conveniencia para evaluaciÃ³n rÃ¡pida
def evaluar_rapido(y_true, y_pred, nombre="Modelo"):
    """
    EvaluaciÃ³n rÃ¡pida con tabla bÃ¡sica.
    """
    return evaluar_modelo_regresion(y_true, y_pred, nombre, 
                                  mostrar_estadisticas=False, 
                                  mostrar_plot=False)


# FunciÃ³n de conveniencia para comparaciÃ³n rÃ¡pida
def comparar_rapido(modelos_dict, y_true, ordenar_por='RÂ²'):
    """
    ComparaciÃ³n rÃ¡pida sin grÃ¡ficos.
    """
    return comparar_modelos_regresion(modelos_dict, y_true,
                                    mostrar_plots=False,
                                    ordenar_por=ordenar_por)
