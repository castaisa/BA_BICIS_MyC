# üß† Red Neuronal Multioutput - Gu√≠a de Uso

Este m√≥dulo proporciona una funci√≥n completa para entrenar redes neuronales multioutput en PyTorch con m√∫ltiples mejoras opcionales.

## üöÄ Funci√≥n Principal

### `neural_network_multioutput()`

Red neuronal multioutput con arquitectura personalizable y mejoras opcionales.

**Retorna**: `(y_pred, train_losses, val_losses)` - predicciones y historiales de p√©rdidas

### `plot_multiple_loss_histories()`

Grafica m√∫ltiples historiales de training y validation loss para comparar modelos.

### `compare_model_architectures()`

Funci√≥n de conveniencia para entrenar y comparar m√∫ltiples arquitecturas autom√°ticamente.

#### ‚öôÔ∏è Par√°metros

| Par√°metro | Tipo | Default | Descripci√≥n |
|-----------|------|---------|-------------|
| `x_train` | array | - | Features de entrenamiento |
| `y_train` | array | - | Targets de entrenamiento (multioutput) |
| `x_val` | array | - | Features de validaci√≥n |
| `y_val` | array | - | Targets de validaci√≥n (multioutput) |
| `hidden_sizes` | list | `[128, 64, 32]` | Tama√±os de capas ocultas |
| `learning_rate` | float | `0.001` | Tasa de aprendizaje |
| `epochs` | int | `100` | N√∫mero m√°ximo de √©pocas |
| `batch_size` | int | `32` | Tama√±o del batch |
| `dropout_rate` | float/None | `None` | Tasa de dropout (ej: 0.2) |
| `early_stopping_patience` | int/None | `None` | Paciencia para early stopping (ej: 10) |
| `normalize` | bool | `False` | Si normalizar datos |
| `verbose` | bool | `False` | Si mostrar progreso |
| `use_adam` | bool | `True` | Usar Adam (True) o SGD (False) |
| `l2_regularization` | float/None | `None` | Weight decay L2 (ej: 1e-4) |
| `use_scheduler` | bool | `False` | Usar ReduceLROnPlateau |
| `plot_losses` | bool | `False` | Generar gr√°fico training vs validation loss |

## üìä Ejemplos de Uso

### 1. Configuraci√≥n B√°sica (M√≠nima)
```python
# Retorna predicciones y historiales
y_pred, train_losses, val_losses = neural_network_multioutput(x_train, y_train, x_val, y_val)
```

### 2. Con Normalizaci√≥n y Dropout
```python
y_pred, train_losses, val_losses = neural_network_multioutput(
    x_train, y_train, x_val, y_val,
    dropout_rate=0.2,
    normalize=True,
    verbose=True
)
```

### 3. Configuraci√≥n Completa (Todas las Mejoras)
```python
y_pred, train_losses, val_losses = neural_network_multioutput(
    x_train, y_train, x_val, y_val,
    hidden_sizes=[256, 128, 64, 32],
    learning_rate=0.0005,
    epochs=200,
    batch_size=64,
    dropout_rate=0.3,
    early_stopping_patience=15,
    normalize=True,
    verbose=True,
    use_adam=True,
    l2_regularization=1e-4,
    use_scheduler=True,
    plot_losses=True  # Genera gr√°fico de p√©rdidas
)
```

### 4. Comparar M√∫ltiples Modelos Manualmente
```python
# Entrenar varios modelos
y_pred1, train1, val1 = neural_network_multioutput(x_train, y_train, x_val, y_val, hidden_sizes=[64, 32])
y_pred2, train2, val2 = neural_network_multioutput(x_train, y_train, x_val, y_val, hidden_sizes=[128, 64])
y_pred3, train3, val3 = neural_network_multioutput(x_train, y_train, x_val, y_val, hidden_sizes=[256, 128])

# Comparar en un solo gr√°fico
plot_multiple_loss_histories(
    [(train1, val1), (train2, val2), (train3, val3)],
    ['Modelo Peque√±o', 'Modelo Mediano', 'Modelo Grande'],
    subplot_layout='together'
)

# Comparar en subplots separados
plot_multiple_loss_histories(
    [(train1, val1), (train2, val2), (train3, val3)],
    ['Modelo Peque√±o', 'Modelo Mediano', 'Modelo Grande'],
    subplot_layout='separate'
)
```

### 5. Comparaci√≥n Autom√°tica de Arquitecturas
```python
# Definir arquitecturas a comparar
architectures = [
    [64, 32],
    [128, 64, 32],
    [256, 128, 64],
    [512, 256, 128]
]

# Entrenar y comparar autom√°ticamente
results = compare_model_architectures(
    x_train, y_train, x_val, y_val,
    architectures,
    epochs=100,
    dropout_rate=0.2,
    normalize=True,
    verbose=False
)
```

### 4. Usando SGD en lugar de Adam
```python
y_pred = neural_network_multioutput(
    x_train, y_train, x_val,
    learning_rate=0.01,  # LR m√°s alto para SGD
    use_adam=False,
    verbose=True
)
```

## üéØ Configuraciones Recomendadas

### Para Datasets Grandes (>10k muestras)
```python
y_pred, train_losses, val_losses = neural_network_multioutput(
    x_train, y_train, x_val, y_val,
    hidden_sizes=[512, 256, 128, 64],
    learning_rate=0.001,
    epochs=200,
    batch_size=128,
    dropout_rate=0.3,
    early_stopping_patience=15,
    normalize=True,
    verbose=True,
    use_adam=True,
    l2_regularization=1e-4,
    use_scheduler=True,
    plot_losses=True
)
```

### Para Datasets Peque√±os (<1k muestras)
```python
y_pred, train_losses, val_losses = neural_network_multioutput(
    x_train, y_train, x_val, y_val,
    hidden_sizes=[128, 64],
    learning_rate=0.001,
    epochs=100,
    batch_size=32,
    dropout_rate=0.2,
    early_stopping_patience=10,
    normalize=True,
    verbose=True,
    l2_regularization=1e-5,
    plot_losses=True
)
```

### Comparaci√≥n R√°pida de Arquitecturas
```python
architectures = [[64, 32], [128, 64], [256, 128, 64]]
results = compare_model_architectures(
    x_train, y_train, x_val, y_val,
    architectures,
    epochs=100,
    dropout_rate=0.2,
    normalize=True
)
```

## üõ†Ô∏è Mejoras Implementadas

### ‚úÖ Optimizadores
- **Adam** (default): Adaptive learning rate, funciona bien en la mayor√≠a de casos
- **SGD**: Gradient descent cl√°sico, a veces m√°s estable

### ‚úÖ Regularizaci√≥n
- **Dropout**: Previene overfitting desactivando neuronas aleatoriamente
- **L2 Regularization**: Weight decay, penaliza pesos grandes

### ‚úÖ Entrenamiento Inteligente
- **Early Stopping**: Para autom√°ticamente si no hay mejora
- **Learning Rate Scheduler**: Reduce LR cuando se estanca
- **Normalizaci√≥n**: StandardScaler para features y targets
- **Validation Loss**: Monitoreo continuo durante entrenamiento

### ‚úÖ Visualizaci√≥n y Comparaci√≥n
- **Gr√°fico de p√©rdidas**: Training vs Validation loss por √©poca
- **Comparaci√≥n m√∫ltiple**: Gr√°ficos juntos o separados
- **Comparaci√≥n autom√°tica**: Entrenar y comparar arquitecturas
- **Marcadores especiales**: Early stopping, mejor modelo
- **Estad√≠sticas**: M√©tricas finales y rankings autom√°ticos

### ‚úÖ Arquitectura Flexible
- **Capas ocultas personalizables**: Ajusta `hidden_sizes`
- **Batch size variable**: Para optimizar memoria y velocidad
- **Device selection**: CPU o CUDA

## üîß Tips de Uso

1. **Siempre empieza simple**: Usa configuraci√≥n b√°sica primero
2. **Normaliza tus datos**: Especialmente importante para redes neuronales
3. **Usa early stopping**: Evita overfitting autom√°ticamente
4. **GPU autom√°tica**: La funci√≥n usa GPU si est√° disponible, sino CPU
5. **Experimenta con arquitectura**: M√°s capas/neuronas para datos complejos
6. **Ajusta learning rate**: 0.001 es buen inicio, 0.01 para SGD

## üö® Notas Importantes

- **Nuevo retorno**: Ahora retorna `(y_pred, train_losses, val_losses)` en lugar de solo `y_pred`
- **Breaking change**: Actualizar c√≥digo existente para manejar m√∫ltiples valores de retorno
- **Device autom√°tico**: Usa CUDA si est√° disponible, sino CPU
- **Validation Loss**: Ahora requiere `y_val` para monitoreo durante entrenamiento
- **Early Stopping**: Basado en validation loss (m√°s robusto que training loss)
- **Scheduler**: Tambi√©n usa validation loss para ajustar learning rate
- **Gr√°ficos**: Visualizaci√≥n autom√°tica de curvas de aprendizaje
- **Comparaci√≥n**: Nuevas funciones para comparar m√∫ltiples modelos f√°cilmente
- Las predicciones se redondean y limitan a valores no negativos
- El modelo guarda autom√°ticamente el mejor estado con early stopping
- Todas las mejoras son **opcionales** - puedes usar solo las que necesites
- Compatible con datos multioutput (m√∫ltiples targets simult√°neos)

## üìã Dependencias Requeridas

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import numpy as np
```

## üé® Clase MultiOutputMLP

La funci√≥n utiliza internamente la clase `MultiOutputMLP` que implementa:
- Capas fully connected con ReLU
- Dropout opcional entre capas
- Arquitectura flexible

Ejemplo de uso directo de la clase:
```python
model = MultiOutputMLP(
    input_size=10, 
    hidden_sizes=[128, 64, 32], 
    output_size=3, 
    dropout_rate=0.2
)
```
